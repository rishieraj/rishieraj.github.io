<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Rishie Raj</title>

    <meta name="author" content="Rishie Raj">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Rishie Raj
                </p>
                <p>
                  I am a graduate researcher, currently working on audio-visual LLMs and multi-modal agentic systems.
                  I graduated with a master's degree in Robotics from the <a href="https://umd.edu/">University of Maryland, College Park</a>, where most of my coarsework was in Computer Vision (<a href="https://www.cs.umd.edu/class/spring2024/cmsc472/">CMSC472</a>) and Multi-modal Learning (<a href="https://jbhuang0604.github.io/teaching/CMSC848K/">CMSC848K</a>, <a href="https://www.cs.umd.edu/class/spring2025/cmsc848m/">CMSC848M</a>).
                </p>
                <p style="text-align:center">
                  <a href="mailto:rishie.raj27@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/RishieRaj-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=jVSO2roAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/rishieraj/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/rishieraj">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/RishieRaj.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/RishieRaj.png" class="hoverZoomLink"></a>
              </td>
            </tr>Â 
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests primarily include multi-modal learning, audio-visual learning, and agentic systems. Below are some of my recent works.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='images/aura_picture.png' width="160" alt="Project Image">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://rishieraj.github.io/aura-project-page/">
                  <span class="papertitle">AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning</span>
                </a>
                <br>
                  <a href="#">Siminfar Samakoush Galougah</a>,
                  <strong>Rishie Raj</strong>,
                  <a href="https://schowdhury671.github.io/">Sanjoy Chowdhury</a>,
                  <a href="https://sayannag.github.io/">Sayan Nag</a>,
                  <a href="https://users.umiacs.umd.edu/~ramanid/">Ramani Duraiswami</a>
                <br>
                  <em>arXiv</em>, 2025
                <br>
                  <a href="https://rishieraj.github.io/aura-project-page/">project page</a>
                  /
                  <a href="http://arxiv.org/abs/2508.07470">arXiv</a>
                <p>
                  The first MCQ-type benchmark designed to evaluate SOTA Audio-Visual Large Language Models and Omni-Modal Language Models on fine-grained cognitive tasks.
                </p>
              </td>
            </tr>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='images/fastar_picture.png' width="160" alt="Project Image">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://github.com/tianyi-lab/FaSTAR">
                  <span class="papertitle">FaSTA*: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing</span>
                </a>
                <br>
                  <a href="https://advaitgupta.github.io/">Advait Gupta</a>,
                  <strong>Rishie Raj</strong>,
                  <a href="https://dangne.github.io/">Dang Nguyen</a>,
                  <a href="https://tianyizhou.github.io/">Tianyi Zhou</a>
                <br>
                  <em>arXiv</em>, 2025
                <br>
                  <a href="https://github.com/tianyi-lab/FaSTAR">code</a>
                  /
                  <a href="http://arxiv.org/abs/2508.07470">arXiv</a>
                <p>
                  A cost-efficient neurosymbolic agent with fast-slow planning to address challenging multi-turn image editing tasks.
                </p>
              </td>
            </tr>
          </tbody></table>

					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Other Projects</h2>
                <p>
                  Here are some other projects that I've worked on as part of my coursework or personal interest.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='images/beyond_text_picture.png' width="160" alt="Project Image">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="data/beyond_text_paper.pdf">
                  <span class="papertitle">Beyond Text: An LLM Agent Approach to Multimodal Reference-Guided Image Editing</span>
                </a>
                <br>
                  <a href="https://advaitgupta.github.io/">Advait Gupta</a>,
                  <strong>Rishie Raj</strong>,
                  <a href="https://www.linkedin.com/in/nithin-murugan/">Nithin Skantha Murugan</a>
                <br>
                  <em><a href="https://www.cs.umd.edu/class/spring2025/cmsc848m/">CMSC848M</a></em>, 2025
                <br>
                  <a href="data/beyond_text_paper.pdf">paper</a>
                <p>
                  A novel LLM-driven agentic framework that handles indirect, multimodal instructions without dedicated agent retraining.
                </p>
              </td>
            </tr>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='images/zero_shot_detection_picture.png' width="160" alt="Project Image">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="data/zero_shot_detection_paper.pdf">
                  <span class="papertitle">A Novel Approach for Detecting AI-Generated Images in Zero-Shot Setting</span>
                </a>
                <br>
                  <a href="https://www.linkedin.com/in/nithin-murugan/">Nithin Skantha Murugan</a>,
                  <a href="https://www.linkedin.com/in/kpratardan/">Krishna Taduri</a>,
                  <strong>Rishie Raj</strong>
                <br>
                  <em><a href="https://jbhuang0604.github.io/teaching/CMSC848K/">CMSC848K</a></em>, 2024
                <br>
                  <a href="data/zero_shot_detection_paper.pdf">paper</a>
                <p>
                  A novel approach for detecting AI-generated images based on cross-perplexity and perplexity computations using autoregressive image generation models.
                </p>
              </td>
            </tr>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='images/object_detection_picture.png' width="160" alt="Project Image">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="data/object_detection_paper.pdf">
                  <span class="papertitle">Parameterized Defogging Network for Object Detection in Adverse Weather Conditions</span>
                </a>
                <br>
                  <strong>Rishie Raj</strong>,
                  <a href="https://uthappa13.github.io/portfolio/">Uthappa Madettira</a>,
                  <a href="https://www.linkedin.com/in/nathan-nussbaumer-0430021b0/">Nathan Nussbaumer</a>,
                  <a href="https://sahajsingh10.github.io/">Sahaj Singh</a>,
                  <a href="https://www.linkedin.com/in/lucas-leitao-72ab51288/">Lucas Leitao</a>
                <br>
                  <em><a href="https://www.cs.umd.edu/class/spring2024/cmsc472/">CMSC472</a></em>, 2024
                <br>
                  <a href="data/object_detection_paper.pdf">paper</a>
                  /
                  <a href="data/object_detection_poster.pdf">poster</a>
                <p>
                  A small convolutional neural network model designed to predict the parameters of differentiable image processing functions with the aim to defog the input images.
                </p>
              </td>
            </tr>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='images/segment_picture.png' width="160" alt="Project Image">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="data/segment_paper.pdf">
                  <span class="papertitle">Real Time Semantic Segmentation using Efficient Neural Network</span>
                </a>
                <br>
                  <strong>Rishie Raj</strong>,
                  <a href="https://uthappa13.github.io/portfolio/">Uthappa Madettira</a>
                <br>
                  <em>ENPM673</em>, 2024
                <br>
                  <a href="data/segment_paper.pdf">paper</a>
                  /
                  <a href="data/segment_slides.pdf">slides</a>
                <p>
                  An implementation of Efficient Neural Network (E-Net), finetuned for real-time semantic segmentation, which is specifically designed for tasks requiring low latency operation.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
